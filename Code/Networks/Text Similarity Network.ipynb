{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb2729c-b303-4b22-ad12-5713ff1b107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import ast\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0114ff-44a3-48b0-890a-ca56ba83d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('spanish'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca7daef-adbd-415f-9c43-6c02ed5beaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7723c716-6fb2-4f70-a242-6c55abe6f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowenyi/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d55cff-abe8-47ba-87a0-b6e8f1455d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = pd.read_csv(\"en_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421b595f-8afd-4856-b160-91e4b2d8afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_data = pd.read_csv(\"es_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff609fe3-a700-4653-b5cf-cb43246c3b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'lang', 'epoch', 'hashtags', 'links', 'replyCount',\n",
       "       'retweetCount', 'likeCount', 'quoteCount', 'conversationId',\n",
       "       'mentionedUsers', 'id_str', 'followersCount', 'friendsCount',\n",
       "       'statusesCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e191c6-a1a8-4a2e-bbf4-5102edc02587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(field):\n",
    "    return ast.literal_eval(field)\n",
    "\n",
    "en_data['hashtags'] = en_data['hashtags'].apply(convert_format)\n",
    "en_data['links'] = en_data['links'].apply(convert_format)\n",
    "en_data['mentionedUsers'] = en_data['mentionedUsers'].apply(convert_format)\n",
    "\n",
    "es_data['hashtags'] = es_data['hashtags'].apply(convert_format)\n",
    "es_data['links'] = es_data['links'].apply(convert_format)\n",
    "es_data['mentionedUsers'] = es_data['mentionedUsers'].apply(convert_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfa782-9348-4ad0-9d49-f267525b3bab",
   "metadata": {},
   "source": [
    "### Text similarity network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4b1aa-786a-4a67-a067-1be563ed75be",
   "metadata": {},
   "source": [
    "#### 1. Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15180798-15e6-42bd-b5f7-b5b07c04e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove emojis\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    # Tokenize and remove stopwords\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b09a0c-978b-42ef-8ddf-7a73ac061f73",
   "metadata": {},
   "source": [
    "#### 2. Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9864cf-8a10-4bd4-bc68-19b7d3a2864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowenyi/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76f53342-722a-45c2-9e3e-a916669192e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_text_similarity_network(dataframe, threshold=0.90):\n",
    "    # Step 1: Clean the text column\n",
    "    dataframe['cleaned_text'] = dataframe['text'].apply(clean_text)\n",
    "    \n",
    "    # Step 2: Filter out tweets with fewer than 4 words\n",
    "    dataframe = dataframe[dataframe['cleaned_text'].apply(lambda x: len(x.split()) >= 4)]\n",
    "    \n",
    "    # Step 3: Group tweets by user\n",
    "    user_text_map = dataframe.groupby('id_str')['cleaned_text'].apply(list)\n",
    "    user_text_map = {user: texts for user, texts in user_text_map.items() if len(texts) >= 10}\n",
    "\n",
    "    # Step 4: Generate embeddings for each user\n",
    "    user_embeddings = {}\n",
    "    for i, (user, texts) in enumerate(user_text_map.items()):\n",
    "        # Encode all texts for the user\n",
    "        embeddings = model.encode(texts, device='cuda', batch_size=64)\n",
    "        # Average embeddings to get a single user-level vector\n",
    "        user_embeddings[user] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    # Step 5: Compute cosine similarity matrix\n",
    "    user_ids = list(user_embeddings.keys())\n",
    "    user_vectors = np.array(list(user_embeddings.values()))\n",
    "    similarity_matrix = cosine_similarity(user_vectors)\n",
    "\n",
    "    # Step 6: Build the user-user similarity graph\n",
    "    G = nx.Graph()\n",
    "    for i, user1 in enumerate(user_ids):\n",
    "        for j, user2 in enumerate(user_ids):\n",
    "            if i != j and similarity_matrix[i, j] > threshold:\n",
    "                G.add_edge(user1, user2, weight=similarity_matrix[i, j])\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a09b1da-9cd3-4add-a511-0e5f3cdb4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text_graph = construct_text_similarity_network(en_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b9ddacf-ed39-4b9b-85ae-7f070269b11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4058"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text_graph.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc421e8-28ec-4e98-b94f-4d20cf901257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c44ab80-3aff-4361-887c-4ee3e18596f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515595"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_text_graph = construct_text_similarity_network(es_data)\n",
    "es_text_graph.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4aeb2-b48b-475f-9c4a-023a8f63817d",
   "metadata": {},
   "source": [
    "#### We need a higher similarity threshold for Spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ab4dab-b1ef-4c0f-984b-a02e72dc0204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14280"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_text_graph = construct_text_similarity_network(es_data,0.95)\n",
    "es_text_graph.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1447b2b9-73d0-4f77-b4ec-201090ecf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(es_text_graph, open('es_text_graph.gpickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6741074e-1fff-4866-8ea3-6fc286f0d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es_text_graph = pickle.load(open('es_text_graph.gpickle', 'rb'))\n",
    "# en_text_graph = pickle.load(open('en_text_graph.gpickle', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "574acec7-7fc0-4b96-bafc-1aeb6c535fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(en_text_graph, \"en_text_graph_new.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcac17c8-d299-4836-8df9-a496d6e490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(es_text_graph, \"es_text_graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129d58e-fdfa-4db4-891b-69007c4f981b",
   "metadata": {},
   "source": [
    "### 3. Cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc6d960-4134-4992-aba2-b70e6edc0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_clusters = pd.read_csv(\"en_text_sim.csv\")\n",
    "es_clusters = pd.read_csv(\"es_text_sim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd49159-6db7-4091-9cd0-a38ab4ef6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from collections import Counter\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "\n",
    "def get_top_topics(cluster_df, data_df):\n",
    "    \"\"\"\n",
    "    Analyzes the top topics in a cluster using BERTopic.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame representing the cluster, containing user IDs in the 'Id' column and 'eigencentrality'.\n",
    "    - data_df: DataFrame containing the raw tweet data, with user IDs in 'id_str' and tweets in 'text'.\n",
    "    \"\"\"\n",
    "    # Extract tweets for users in the cluster\n",
    "    texts = []\n",
    "    for _, row in cluster_df.iterrows():\n",
    "        user = row['Id']\n",
    "        user_data = data_df[data_df['id_str'] == user]\n",
    "        \n",
    "        for _, tweet_row in user_data.iterrows():\n",
    "            tweet = tweet_row['text']\n",
    "            if isinstance(tweet, str) and len(tweet) > 0:\n",
    "                texts.append(tweet)\n",
    "\n",
    "    # Check if texts are non-empty\n",
    "    if not texts:\n",
    "        print(\"No texts available in this cluster.\")\n",
    "        return\n",
    "\n",
    "    # Apply BERTopic\n",
    "    representation_model = KeyBERTInspired()\n",
    "    topic_model = BERTopic(language=\"multilingual\", representation_model=representation_model)\n",
    "    topics, probs = topic_model.fit_transform(texts)\n",
    "\n",
    "    # Get the most common topics\n",
    "    print(topic_model.get_topic(10))\n",
    "    \n",
    "    # Cluster metrics\n",
    "    print(f\"\\nCluster size: {cluster_df.shape[0]}\")\n",
    "    print(f\"Avg Eigenvector Centrality (EC): {cluster_df['eigencentrality'].mean()}\")\n",
    "    print(\"Top 2 Nodes by EC:\")\n",
    "    print(cluster_df.nlargest(2, 'eigencentrality')[['Id', 'eigencentrality']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9555995-bfad-4c32-88bc-5740397665dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abortion', 0.60028183), ('abortions', 0.5542473), ('fetuses', 0.37792125), ('pregnancy', 0.33204708), ('unborn', 0.3180533), ('roe', 0.30129647), ('reproductive', 0.27235496), ('neutered', 0.26331052), ('newsweek', 0.24960382), ('fertilization', 0.24601033)]\n",
      "\n",
      "Cluster size: 242\n",
      "Avg Eigenvector Centrality (EC): 0.20524888016528928\n",
      "Top 2 Nodes by EC:\n",
      "                       Id  eigencentrality\n",
      "3   '1607179494267453440'           1.0000\n",
      "48  '1323861935385800705'           0.9997\n"
     ]
    }
   ],
   "source": [
    "get_top_topics(en_clusters[en_clusters['modularity_class']==0], en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66a0830a-d077-4b4c-b106-d075f097f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bullet', 0.4730395), ('gunfire', 0.47082508), ('assassination', 0.4590474), ('shooting', 0.44375038), ('shot', 0.41272205), ('shooter', 0.39944214), ('gunmen', 0.36273488), ('pennsylvania', 0.34602836), ('sniper', 0.34067425), ('shoot', 0.33194247)]\n",
      "\n",
      "Cluster size: 155\n",
      "Avg Eigenvector Centrality (EC): 0.1851809741935484\n",
      "Top 2 Nodes by EC:\n",
      "             Id  eigencentrality\n",
      "90   '14662354'         0.846619\n",
      "107  '18956073'         0.824424\n"
     ]
    }
   ],
   "source": [
    "get_top_topics(en_clusters[en_clusters['modularity_class']==6], en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc75f32c-ef85-4c21-8804-72bf566596d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump2024tosaveamerica', 0.5242352), ('trump2024', 0.5153598), ('donaldjtrumpjr', 0.46091014), ('trump', 0.4561502), ('donald', 0.443261), ('realdonaldtrump', 0.41920254), ('trumpanzees', 0.40702057), ('trumpmeltdown', 0.39976558), ('trumps', 0.39621258), ('melania', 0.38993993)]\n",
      "\n",
      "Cluster size: 103\n",
      "Avg Eigenvector Centrality (EC): 0.03263343689320389\n",
      "Top 2 Nodes by EC:\n",
      "              Id  eigencentrality\n",
      "326  '576975457'         0.259018\n",
      "23   '626760400'         0.224929\n"
     ]
    }
   ],
   "source": [
    "get_top_topics(en_clusters[en_clusters['modularity_class']==10], en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b40010-205b-4278-9868-d1c7e6d10cf6",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a163287-6747-4c03-a17b-6580efcb1bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 0.74134916), ('trumpmeltdown', 0.6016231), ('trump2024', 0.50835246), ('obama', 0.5025191), ('discurso', 0.3884017), ('retórica', 0.36591637), ('mítines', 0.3581076), ('libertario', 0.35769534), ('trumpisunfitforoffice', 0.3180873), ('audiencia', 0.31041327)]\n",
      "\n",
      "Cluster size: 332\n",
      "Avg Eigenvector Centrality (EC): 0.15878569879518073\n",
      "Top 2 Nodes by EC:\n",
      "             Id  eigencentrality\n",
      "76   '16676396'         1.000000\n",
      "42  '133945128'         0.959601\n"
     ]
    }
   ],
   "source": [
    "get_top_topics(es_clusters[es_clusters['modularity_class']==0], es_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67107889-738f-47c3-bb5c-370e95e35e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fbi', 0.44429463), ('trump', 0.3996332), ('sospechoso', 0.38954106), ('identificó', 0.34184307), ('sospechosos', 0.32849008), ('identificar', 0.27406016), ('atacante', 0.2710208), ('identifica', 0.25448278), ('crooks', 0.24684718), ('atentado', 0.23638453)]\n",
      "\n",
      "Cluster size: 281\n",
      "Avg Eigenvector Centrality (EC): 0.2314304412811388\n",
      "Top 2 Nodes by EC:\n",
      "              Id  eigencentrality\n",
      "150  '520653311'         0.909223\n",
      "5    '200267797'         0.895046\n"
     ]
    }
   ],
   "source": [
    "get_top_topics(es_clusters[es_clusters['modularity_class']==72], es_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12709d50-6cfe-4513-9852-a627c3e96eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
